{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"AI-Sweden-Models/gpt-sw3-1.3b\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "prompt = \"hur är det med dig\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/mnt/d1/KEX/.myenv/GPT-sw3-126m.my_own\")\n",
    "model.to(device)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "generated_token_ids = model.generate(\n",
    "    inputs=input_ids,\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=1,\n",
    ")[0]\n",
    "\n",
    "generated_text = tokenizer.decode(generated_token_ids)    \n",
    "\n",
    "print(generated_text)\n",
    "\n",
    "#funkar typ ju!!!!! vet inte varför den tar med indata, kanske något litet fel i träningen. Får kolla på det den."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
