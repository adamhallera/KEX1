{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pb\n",
    "\n",
    "\n",
    "\n",
    "def taBort(frånFil, tillFil):\n",
    "    datasmall = pb.read_excel(frånFil)\n",
    "\n",
    "    missing_percentage = (datasmall.isnull().sum() / len(datasmall)) * 100\n",
    "\n",
    "    cols_to_drop = missing_percentage[missing_percentage >= 99].index\n",
    "\n",
    "    datasmall_filtered = datasmall.drop(columns=cols_to_drop)\n",
    "\n",
    "\n",
    "    print(\"Shape after filtering:\", datasmall_filtered.shape)\n",
    "\n",
    "    datasmall_filtered.to_excel(tillFil, index=False)\n",
    "\n",
    "\n",
    "taBort('LitenData.xlsx','LitenData_rensad.xlsx')\n",
    "print(1)\n",
    "taBort(\"LitenDataSorre.xlsx\",\"LitenDataSorre_rensad.xlsx\")\n",
    "print(1)\n",
    "taBort(\"LitenDataMyckeStorre.xlsx\",\"LitenDataMyckeStorre_rensad.xlsx\")\n",
    "print(1)\n",
    "taBort(\"updFAKSexp.xlsx\",\"updFAKSexp_rensad.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after filtering: (100, 241)\n",
      "{'input_ids': tensor([    3,     2, 15088,  ...,  2265, 63443,     2]), 'labels': tensor([    3,     2, 15088,  ...,  2265, 63443,     2]), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1])}\n",
      "Maximum length of input_ids: 1748\n"
     ]
    }
   ],
   "source": [
    "import pandas as pb\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerFast, pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "model_name = \"AI-Sweden-Models/gpt-sw3-126m\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "datasmall = pb.read_excel('testtest.xlsx')\n",
    "\n",
    "missing_percentage = (datasmall.isnull().sum() / len(datasmall)) * 100\n",
    "\n",
    "cols_to_drop = missing_percentage[missing_percentage >= 97].index\n",
    "\n",
    "datasmall = datasmall.drop(columns=cols_to_drop)\n",
    "\n",
    "\n",
    "print(\"Shape after filtering:\", datasmall.shape)\n",
    "\n",
    "\n",
    "\n",
    "# tokaniserar en rad och kollar hur många tokens över.\n",
    "# tokaniserar kolumn FY och tar bort bakifrån antal tokens vi vill ah bort tills den e tom\n",
    "# tar all data från excel utom kolumn FY och tokaniserar men byter ut kolumn FY med det vi redan tokaniserat och tagit bort från\n",
    "# sammanställer\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "for i in range(datasmall.shape[0]):\n",
    "    tokens = tokenizer.encode_plus(str(datasmall.iloc[i, -1]), return_tensors='pt')\n",
    "    input_ids = tokens['input_ids'][0].tolist()\n",
    "    tokens_remove_list = len(input_ids) - 2024\n",
    "    if tokens_remove_list < 0:\n",
    "        tokens_remove_list = 0\n",
    "\n",
    "    # Remove tokens from the end until the length matches 2024\n",
    "    while len(input_ids) > 2024:\n",
    "        input_ids.pop()\n",
    "\n",
    "    # Update the last column data in the dataframe with the decoded string\n",
    "    datasmall.iloc[i, -1] = tokenizer.decode(input_ids)\n",
    "    \n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "input_tokens = datasmall.iloc[:, 1:].values.tolist()  \n",
    "output_tokens = datasmall.iloc[:, 0].values.tolist()\n",
    "\n",
    "output_tokens = [str(value) for value in output_tokens]\n",
    "\n",
    "processed_texts = []\n",
    "\n",
    "for text in output_tokens:\n",
    "    SokOrd = {'jmf','Jmf','JMF','jämfört','Jämfört','jämförelse','Jämförelse','tidigare','Tidigare','föregående','Föregående','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022','2023','2024'}\n",
    "\n",
    "    for i in range(0, 3):\n",
    "        for s in SokOrd:\n",
    "            head, sep, tail = text.partition(s)\n",
    "            if head == text:\n",
    "                head = \"\" \n",
    "            txt = head[::-1]\n",
    "\n",
    "            for tecken in txt:\n",
    "                if tecken == '.':\n",
    "                    head2, sep2, tail2 = tail.partition('.')\n",
    "                    Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                    #Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                    new_txt = Second_tail[::-1]\n",
    "                    text = new_txt + tail2\n",
    "                    break\n",
    "                \n",
    "                if tecken == '(':\n",
    "                    head2, sep2, tail2 = tail.partition(')')\n",
    "                    Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                    Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                    new_txt = Second_tail[::-1]\n",
    "                    text = new_txt + tail2\n",
    "                    break\n",
    "\n",
    "                if tecken == '?':\n",
    "                    head2, sep2, tail2 = tail.partition('.')\n",
    "                    Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                    Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                    new_txt = Second_tail[::-1]\n",
    "                    text = new_txt + tail2\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "    processed_texts.append(text.strip()) \n",
    "\n",
    "output_tokens = processed_texts\n",
    "\n",
    "par_tokens = [(output_tokens[i], input_tokens[i])for i in range(len(input_tokens))]\n",
    "\n",
    "\n",
    "train_texts, val_and_test_texts = train_test_split(par_tokens, test_size=0.2, random_state=42) \n",
    "val_texts, test_texts = train_test_split(val_and_test_texts, test_size=0.5, random_state=42)\n",
    "\n",
    "output_tokens_from_par_train_small = [str(item[0]) for item in train_texts]\n",
    "output_tokens_from_par_val_small = [str(item[0]) for item in val_texts]\n",
    "output_tokens_from_par_test_small = [str(item[0]) for item in test_texts]\n",
    "input_tokens_from_par_train_small = [str(item[1]) for item in train_texts]\n",
    "input_tokens_from_par_val_small = [str(item[1]) for item in val_texts]\n",
    "input_tokens_from_par_test_small = [str(item[1]) for item in test_texts]\n",
    "\n",
    "formatted_data_train = [f\"<|endoftext|><s>User: Skriv en patientjornal efter en ultraljudsundersökning utifrån dessa värden: {input_token}<s>Bot:{output_token}<s>\" \n",
    "                        for input_token, output_token in zip(input_tokens_from_par_train_small, output_tokens_from_par_train_small)]\n",
    "\n",
    "formatted_data_val = [f\"<|endoftext|><s>User: Skriv en patientjornal efter en ultraljudsundersökning utifrån dessa värden: {input_token}<s>Bot:{output_token}<s>\" \n",
    "                      for input_token, output_token in zip(input_tokens_from_par_val_small, output_tokens_from_par_val_small)]\n",
    "\n",
    "formatted_data_test = [f\"<|endoftext|><s>User: Skriv en patientjornal efter en ultraljudsundersökning utifrån dessa värden: {input_token}<s>Bot:{output_token}<s>\" \n",
    "                       for input_token, output_token in zip(input_tokens_from_par_test_small, output_tokens_from_par_test_small)]\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, formatted_data, tokenizer):\n",
    "        self.formatted_data = formatted_data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.formatted_data)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        formatted_data = self.formatted_data[idx]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Encode input sequence and labels using tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            formatted_data,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            #padding_side = 'left',\n",
    "            #padding=True,\n",
    "            #truncation=True,\n",
    "            max_length = 1,       #har testat med bara truncation för att se om det är padding. och ja, det är padding som fuckar upp det för modellen.\n",
    "            return_attention_mask=True,\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.flatten(),\n",
    "            \"labels\": inputs.input_ids.flatten(),\n",
    "            \"attention_mask\": inputs.attention_mask.flatten(),\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "AllData_train_small = MyDataset(formatted_data_train, tokenizer)\n",
    "print(AllData_train_small.__getitem__(56))\n",
    "AllData_val_small = MyDataset(formatted_data_val, tokenizer)\n",
    "#random_val, AllData_val_small = train_test_split(AllData_val_small, test_size=0.02, random_state=42)\n",
    "AllData_test_small = MyDataset(formatted_data_test, tokenizer)\n",
    "\n",
    "max_input_length = max(len(example[\"labels\"]) for example in AllData_train_small)\n",
    "\n",
    "print(\"Maximum length of input_ids:\", max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        <|endoftext|><s>        User: Skriv en patientjornal efter en ultraljudsundersökning utifrån dessa värden: ['101 - 153', 1.05, '0,26 - 0,90', 29, 3.0, 198.0, 'Flerstråligt', 'Normal', 'Bicuspid', 'Ja', nan, 35.0, '28 - 39', 16.1, 25.0, 11.5, 2.1, nan, '7,5 - 12,7', 25.0, '24 - 35', 11.5, 18.0, nan, 'Raphe mellan högra och vänstra kuspen. Kraftigt excentrisk, posteriort riktad jet. Vena contracta svår att beräkna. Spektraltät dopplerprofil. Slutdiastolisk hastighet i aorta desc 38 cm/s.', nan, '12 - 20', nan, '11 - 21', 14.0, '12,5 - 20,5', 12.0, '12 - 20', 10.0, '12 - 20', nan, nan, 'God', 'DS', 25.5, 2.17, nan, nan, nan, nan, 77.0, 'MB', 'EM', 171.0, '138 - 194', 1.3, '0,73 - 2,33', 1.32, '0,52 - 1,12', nan, '51 - 71', nan, '52 - 72', 59.0, '55 - 100\\t45 - 54\\t30 - 44\\t< 30', nan, 'Gracil', nan, 'Gracil', nan, 55.0, 'Går själv', nan, 85.0, 'Man', 188.0, 17.0, 7.83, '6 - 13', nan, 'Normal', 25.0, '25 - 43', nan, nan, 20.0, '51 - 83', 14.0, 25.0, '0 - 20\\t20 - 30\\t30 - 40\\t ≥ 40', 11.52073732718894, '16 - 34\\t34 - 41\\t42 - 48\\t ≥ 49', nan, nan, 60.0, 27.65, '22 - 30\\t31 - 33\\t34 - 36\\t ≥ 36', nan, nan, '34 - 74', '55 - 100\\t45 - 54\\t30 - 44\\t< 30', 55.0, 50.0, nan, nan, '7 - 27', 339.0, 224, 156.0, 115, '49 - 115\\t116 - 131\\t132 - 148\\t ≥ 149', nan, 1.1, nan, nan, 11.0, '6 - 12', 1.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5, nan, nan, 3.0, 0.9, nan, 0.4, 0.6, 3.0, 'Grad av AI? VK?', '27 årig man med coarctatio aorte som opererades-88 i Lund med end-to-end-anastomos\\xa0 Även ligering av ductus\\xa0 Bicuspid aortaklaff med ai grad 3/4\\xa0 Kateterburen dil av CoA 2007 och 2011\\xa0', 34.0, 15.66820276497696, '13 - 21', nan, 0.3666666666666667, 'Regelbunden', 1.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, '6 - 12', nan, nan, 134.0, 27.0, 0.09, 17.6, 0.06, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.5, nan, '0 - 26,3', nan, '0 - 2,6', 0.0, nan, nan, 'Nedsatt longitudinell rörlighet, global strain - 15%. Något ökad trabekulering i ffa apikala delar, men även midventrikulärt inferiort. Beräknad auto-EF 50% i biplan. Inga säkra regionala avvikelser.', nan, 39.0, 18.0, '15 - 23\\t24 - 26\\t27 - 29\\t ≥ 30', 90.0, nan, nan, '34 - 74', nan, nan, nan, '7 - 27', nan, 6095, nan, nan, '34 - 74', nan, '60 - 95', nan, nan, '7 - 27', '31 - 67', '49 - 73', '60 - 95', '9 - 29', 180.0, 74.22261538461538, 41.0, 19.0]<s>        Bot:Bicuspid aortaklaff med excentrisk, stor insuff (grad 3/4), ingen stenos. Lätt dilaterad vänster kammare med lätt förtjockat septum och något ökad trabekulering i apikala delar samt midventrikulärt inferiort, EF 50-55%. Lätt dilaterat vänster förmak. Normalstora högersidiga hjärtrum. Normal systolisk högerkammarfunktion. Högst lätt ökad flödeshastighet i aorta desc i anslutning till stentet<s>\n",
      "-------------------------------------------------------------------------\n",
      "tensor([    3,     2, 15088,  ...,   335, 11381,     2])\n",
      "-------------------------------------------------------------------------\n",
      "{'input_ids': [1347, 327, 878, 63458], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [1423], 'attention_mask': [1]}\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "toktorem:  7\n",
      "1757   0   0   0   1757\n",
      "len head2:  0\n",
      "en token borttagen\n",
      "inga token kvar\n",
      "1757   1   0   0   1758\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "1ha\n",
      "         <|endoftext|><s>         User: Skriv en patientjornal efter en ultraljudsundersökning utifrån dessa värden: ['100 - 176', nan, '0,26 - 0,90', 65, 0.0, nan, nan, 'Nedsatt', 'Tricuspid', nan, nan, 25.0, '25 - 37', 15.0, 19.0, 11.4, 1.5, nan, '9,7 - 15,3', nan, '24 - 36', nan, 9.0, nan, nan, 9.0, '12 - 20', 10.0, '11 - 21', 8.0, '12,5 - 20,5', 8.8, '12 - 20', 8.0, '12 - 20', nan, nan, 'Nedsatt', 'KR', 25.7, 1.67, nan, nan, nan, nan, 71.0, 'MU', nan, 106.0, '142 - 258', nan, '0,96 - 1,32', 1.2, '0,52 - 1,12', nan, '53 - 73', nan, '54 - 74', 61.0, '55 - 100\\t45 - 54\\t30 - 44\\t< 30', 'Gracil', 'Gracil', nan, nan, nan, nan, 'Säng', nan, 90.0, 'Kvinna', 159.0, 22.0, 13.17, '6 - 11', nan, 'Nedsatt', 30.0, '22 - 40', nan, nan, 24.0, '73 - 101', 11.0, 21.0, '0 - 20\\t20 - 30\\t30 - 40\\t ≥ 40', 12.5748502994012, '16 - 34\\t34 - 41\\t42 - 48\\t ≥ 49', nan, nan, 40.0, 23.95, '23 - 31\\t32 - 34\\t35 - 37\\t ≥ 37', nan, nan, '29 - 61', '55 - 100\\t45 - 54\\t30 - 44\\t< 30', 60.0, 55.0, nan, nan, '6 - 22', 138.0, 162, 83.0, 95, '43 - 95\\t96 - 108\\t109 - 121\\t ≥ 122', nan, 1.1, nan, nan, 10.0, '6 - 12', 1.0, nan, nan, nan, nan, nan, 96.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.0, nan, nan, 2.0, 0.7, nan, nan, nan, 15.0, 'VKF? HKF? vitier?', 'tid lätt nedsatt syst funktion 2011\\xa0 Kärlsjuk\\xa0 nefrotiskt syndrom-jkontroller njurmedNu rejält med perifera ödem som ej förklaras av lågt albumin\\xa0 Tacksam EKO 7/1', 38.0, 22.75449101796407, 'Referens ej tillgänglig', nan, 0.5, 'Oregelbunden', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, '5 - 12', nan, nan, 151.0, 10.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 2.0, 81.0, '0 - 32,1', 4.5, '0 - 2,8', nan, nan, nan, nan, nan, 42.0, 25.1, '15 - 23\\t24 - 26\\t27 - 29\\t ≥ 30', 65.0, nan, nan, '29 - 61', nan, nan, nan, '6 - 22', nan, 5075, nan, nan, '29 - 61', nan, '50 - 75', nan, nan, '6 - 22', '26 - 58', '52 - 72', '50 - 75', '8 - 24', 70.0, 27.01588235294118, 27.0, 16.0]<s>         Bot:Normalstor vänster kammare med normal systolisk funktion. Lindrigt förstorad höger kammare med måttligt nedsatt systolisk funktion. Lindrigt dilaterade förmak bilateralt. Uttalad pulmonell hypertension. Måttlig tricuspidalisinsufficiens. Lindrig mitralisinsufficiens Lungemboli?<s>\n",
      "2398\n",
      "1454\n",
      "9779\n"
     ]
    }
   ],
   "source": [
    "print(formatted_data_train[5])\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "print(AllData_train_small[5][\"input_ids\"])\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "print(tokenizer.encode_plus(\"Anamnes:\"))\n",
    "print(tokenizer.encode_plus(\"'\"))\n",
    "\n",
    "new_examples_list = []\n",
    "target_number_start = \"Anamnes:\"\n",
    "target_number_end = \"'\"\n",
    "#target_number_start = str(target_number_start)\n",
    "#target_number_end = str(target_number_end)\n",
    "for example in formatted_data_train:\n",
    "    example = tokenizer.encode_plus(example)['input_ids']\n",
    "    #example = str(example)\n",
    "    print(\"1ha\")\n",
    "    if(len(example) > 1750):\n",
    "        # tokens\n",
    "        antal_tokens_toRemove = len(example) - 1750\n",
    "        print(\"toktorem: \", antal_tokens_toRemove)\n",
    "\n",
    "        # string\n",
    "        example = tokenizer.decode(example)\n",
    "        head, sep, tail = example.partition(target_number_start)\n",
    "        stage = sep + tail\n",
    "        head2, sep2, tail2 = stage.partition(target_number_end)\n",
    "        \n",
    "        #tokens\n",
    "        example = tokenizer.encode_plus(example)['input_ids']\n",
    "        head = tokenizer.encode_plus(head)['input_ids']\n",
    "        head2 = tokenizer.encode_plus(head2)['input_ids']\n",
    "        tail2 = tokenizer.encode_plus(tail2)['input_ids']\n",
    "        sep2 = tokenizer.encode_plus(sep2)['input_ids']\n",
    "        print(len(head), \" \", len(head2), \" \", len(tail2), \" \", len(sep2), \" \", len(example))\n",
    "\n",
    "        antal = len(head2)\n",
    "\n",
    "        while len(head2) > (antal - antal_tokens_toRemove):\n",
    "            print(\"len head2: \", len(head2))\n",
    "            head2 = head2[:-1]\n",
    "            print(\"en token borttagen\")\n",
    "            if len(head2) == 0:\n",
    "                head2 = tokenizer.encode_plus(\"nan\")['input_ids']\n",
    "                print(\"inga token kvar\")\n",
    "                break\n",
    "\n",
    "        example = head + head2 + sep2 + tail2\n",
    "        print(len(head), \" \", len(head2), \" \", len(tail2), \" \", len(sep2), \" \", len(example))\n",
    "\n",
    "    new_examples_list.append(tokenizer.decode(example))\n",
    "\n",
    "\n",
    "formatted_data_train = (new_examples_list)\n",
    "\n",
    "\n",
    "print(formatted_data_train[2])\n",
    "print(len(formatted_data_train[2]))\n",
    "print(len(tokenizer.encode_plus(formatted_data_train[0])['input_ids']))\n",
    "print(len(str(tokenizer.encode_plus(formatted_data_train[0])['input_ids'])))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [19937], 'attention_mask': [1]}\n",
      "Maximum length of input_ids: 1755\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode_plus(\"nan\"))\n",
    "max_input_length = max(len(tokenizer.encode_plus(example)['input_ids']) for example in formatted_data_train)                                           \n",
    "print(\"Maximum length of input_ids:\", max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasmall.shape)\n",
    "import torch\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, formatted_data, tokenizer):\n",
    "        self.formatted_data = formatted_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_label_length = 2048  # Maximum allowed length for labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.formatted_data)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        formatted_data = self.formatted_data[idx]\n",
    "\n",
    "        # Encode input sequence and labels using tokenizer\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            formatted_data,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            max_length=self.max_label_length,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        label_length = len(inputs.input_ids.flatten())  # Get the length of labels\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.flatten(),\n",
    "            \"labels\": inputs.input_ids.flatten(),\n",
    "            \"attention_mask\": inputs.attention_mask.flatten(),\n",
    "            \"label_length\": label_length  # Add the label length to the returned dictionary\n",
    "        }\n",
    "\n",
    "# Initialize your dataset\n",
    "AllData_train_small = MyDataset(formatted_data_train, tokenizer)\n",
    "\n",
    "# Count how many labels exceed 2048\n",
    "count_exceeding_2048 = sum(1 for data in AllData_train_small if data[\"label_length\"] > 2048)\n",
    "\n",
    "print(\"Number of labels exceeding 2048:\", count_exceeding_2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = 'Max 6 mm perikardspatium med delvis organiserat innehåll utanför HK och ffa lateralt om VK (inget tidigare EKO tillgängligt för jämförelse, men regress ses vid översiktlig jmf med hjärt-MR 131220). Normalvid vänster kammare med väggtjocklek vid övre normalgräns, måttligt sänkt systolisk funktion (EF ca 40%). Lätt dilaterat vänster förmak. Normalstora högersidiga hjärtrum. Väs normal systolisk högerkammarfunktion. Lindrig mitralisinsuff. Maxsystoliskt PA-tryck ca 30 mmHg.'\n",
    "#text = 'Normalstora hjärtrum. Normal vänster och högerkammarfunktion. EF 60%. Lindriga aorta, mitralis och tricuspidalisinsufficienser. Normalt PA-tryck. Jämfört med föregående us oförändrad bild.'\n",
    "#text = 'Perikardvätska som omger hela hjärtat med max skikttjocklek c:a 15-20 mm inferiort och lateralt om vänster kammare; 10-15 mm utanför höger kammare samt omkring 30 mm utanför höger förmak med påverkan på dess vägg. Ingen tydlig påverkan på kammarnivå. Normalstor vänster kammare med normal systolisk funktion. Normalstor höger kammare med normal systolisk funktion. Normal klaffunktion. Således lindrig hemodynamisk påverkan av perikardvätskan men ingen ekokardiografisk tamponadbild. Ingen säkerställd förändring ses jämfört med undersökning från 2013-12-23. Inga bilder för jämförelse finns i EchoPac-servern från us. 5/1 som anges i remissen - var är den undersökningen utförd?'\n",
    "text = 'Dilaterad höger kammare med nedsatt systolisk funktion. Vänster kammare med normal diameter, lätt septumhypertrofi samt lätt nedsatt global systolisk funktion (50-55%). Lätt septumavplaning. Normalvid v. cava inf, men andningsvariation kan ej bedömas. Systoliskt tryck i lungkretsloppet skattas måttligt förhöjt (minst 45-50 mmHg). Normalstora förmak. Lindriga insufficienser i samtliga klaffar. Jämfört med EKO 2011-10-20 ses nu lägre systolisk höger- och vänsterkammarfunktion. Nytillkommen lätt septumavplaning som kan tyda på nytillkommen PA-tryckstegring (PA-trycket kunde ej skattas 2011). Vid tidpunkten för svarsskrivning finns inga aktuella bilder arkiverade.'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "SokOrd = {'jmf','Jmf','JMF','jämfört','Jämfört','jämförelse','Jämförelse','tidigare','Tidigare','föregående','Föregående','2010-','2011','2012-','2013-','2014-','2015-','2016-','2017-','2018-','2019-','2020-','2021-','2022-','2023-','2024-'}\n",
    "\n",
    "for i in range(0, 3):\n",
    "    for s in SokOrd:\n",
    "        head, sep, tail = text.partition(s)\n",
    "        if head == text:\n",
    "            head = \"\" \n",
    "        txt = head[::-1]\n",
    "\n",
    "        for tecken in txt:\n",
    "            if tecken == '.':\n",
    "                head2, sep2, tail2 = tail.partition('.')\n",
    "                Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                #Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                new_txt = Second_tail[::-1]\n",
    "                text = new_txt + tail2\n",
    "                break\n",
    "            \n",
    "            if tecken == '(':\n",
    "                head2, sep2, tail2 = tail.partition(')')\n",
    "                Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                new_txt = Second_tail[::-1]\n",
    "                text = new_txt + tail2\n",
    "                break\n",
    "\n",
    "            if tecken == '?':\n",
    "                head2, sep2, tail2 = tail.partition('.')\n",
    "                Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                new_txt = Second_tail[::-1]\n",
    "                text = new_txt + tail2\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "print(text.strip())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Max 6 mm perikardspatium med delvis organiserat innehåll utanför HK och ffa lateralt om VK (inget tidigare EKO tillgängligt för jämförelse, men regress ses vid översiktlig jmf med hjärt-MR 131220). Normalvid vänster kammare med väggtjocklek vid övre normalgräns, måttligt sänkt systolisk funktion (EF ca 40%). Lätt dilaterat vänster förmak. Normalstora högersidiga hjärtrum. Väs normal systolisk högerkammarfunktion. Lindrig mitralisinsuff. Maxsystoliskt PA-tryck ca 30 mmHg.'\n",
    "import pandas as pd\n",
    "\n",
    "for tecken in text:\n",
    "    if tecken == 'jmf':\n",
    "        head, sep, tail = text.partition('jmf')\n",
    "        head_inv= head[::-1]\n",
    "        for tckn in head_inv:\n",
    "            if tckn == '(' or '.':\n",
    "                head2, sep2, tail2 = head_inv.partition('(')\n",
    "                if sep2 == '(':\n",
    "                head2, sep2, tail2 = head_inv.partition('.')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datasmall = pd.read_excel('LitenData.xlsx')\n",
    "\n",
    "output_tokens = datasmall.iloc[:, 0].values.tolist()\n",
    "processed_texts = []\n",
    "\n",
    "for text in output_tokens:\n",
    "    SokOrd = {'jmf','Jmf','JMF','jämfört','Jämfört','jämförelse','Jämförelse','tidigare','Tidigare','föregående','Föregående','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022','2023','2024'}\n",
    "\n",
    "    for i in range(0, 3):\n",
    "        for s in SokOrd:\n",
    "            head, sep, tail = text.partition(s)\n",
    "            if head == text:\n",
    "                head = \"\" \n",
    "            txt = head[::-1]\n",
    "\n",
    "            for tecken in txt:\n",
    "                if tecken == '.':\n",
    "                    head2, sep2, tail2 = tail.partition('.')\n",
    "                    Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                    #Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                    new_txt = Second_tail[::-1]\n",
    "                    text = new_txt + tail2\n",
    "                    break\n",
    "                \n",
    "                if tecken == '(':\n",
    "                    head2, sep2, tail2 = tail.partition(')')\n",
    "                    Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                    Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                    new_txt = Second_tail[::-1]\n",
    "                    text = new_txt + tail2\n",
    "                    break\n",
    "\n",
    "                if tecken == '?':\n",
    "                    head2, sep2, tail2 = tail.partition('.')\n",
    "                    Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                    Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                    new_txt = Second_tail[::-1]\n",
    "                    text = new_txt + tail2\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "    processed_texts.append(text.strip())  # Append the processed text to the list, stripping any leading/trailing whitespace\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(processed_texts, columns=['Processed Text'])\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df.to_excel('processed_texts.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Second_sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_tokens[85])\n",
    "print(\"----------------------------\")\n",
    "print(processed_texts[85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "datasmall = pd.read_excel('updFAKSexp_rensad.xlsx')\n",
    "#print(datasmall.shape)\n",
    "\n",
    "\n",
    "'''\n",
    "input_tokens = [(input_text) for input_text in datasmall.iloc[:, 1:].iterrows()]\n",
    "print(len(input_tokens))\n",
    "output_tokens = [(output_text) for output_text in datasmall.iloc[:, 0]]\n",
    "'''\n",
    "\n",
    "input_tokens = datasmall.iloc[:, 1:].values.tolist()  \n",
    "output_tokens = datasmall.iloc[:, 0].values.tolist()\n",
    "output_tokens = [str(value) for value in output_tokens]\n",
    "\n",
    "processed_texts = []\n",
    "\n",
    "for text in output_tokens:\n",
    "    SokOrd = {'jmf','Jmf','JMF','jämfört','Jämfört','jämförelse','Jämförelse','tidigare','Tidigare','föregående','Föregående','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022','2023','2024'}\n",
    "\n",
    "    for i in range(0, 3):\n",
    "        for s in SokOrd:\n",
    "            head, sep, tail = text.partition(s)\n",
    "            if head == text:\n",
    "                head = \"\" \n",
    "            txt = head[::-1]\n",
    "\n",
    "            for tecken in txt:\n",
    "                if tecken == '.':\n",
    "                    head2, sep2, tail2 = tail.partition('.')\n",
    "                    Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                    #Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                    new_txt = Second_tail[::-1]\n",
    "                    text = new_txt + tail2\n",
    "                    break\n",
    "                \n",
    "                if tecken == '(':\n",
    "                    head2, sep2, tail2 = tail.partition(')')\n",
    "                    Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                    Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                    new_txt = Second_tail[::-1]\n",
    "                    text = new_txt + tail2\n",
    "                    break\n",
    "\n",
    "                if tecken == '?':\n",
    "                    head2, sep2, tail2 = tail.partition('.')\n",
    "                    Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                    Second_tail = Second_tail[1:] #ta bort sista mellanslag\n",
    "                    new_txt = Second_tail[::-1]\n",
    "                    text = new_txt + tail2\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "    processed_texts.append(text.strip()) \n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(processed_texts, columns=['Processed Text'])\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "df.to_excel('processed_texts.xlsx', index=False)\n",
    "\n",
    "output_tokens = processed_texts\n",
    "\n",
    "\n",
    "par_tokens = [(output_tokens[i], input_tokens[i])for i in range(len(input_tokens))]\n",
    "\n",
    "train_texts, val_and_test_texts = train_test_split(par_tokens, test_size=0.2, random_state=42) \n",
    "val_texts, test_texts = train_test_split(val_and_test_texts, test_size=0.5, random_state=42)\n",
    "\n",
    "output_tokens_from_par_train_small = [str(item[0]) for item in train_texts]\n",
    "output_tokens_from_par_val_small = [str(item[0]) for item in val_texts]\n",
    "output_tokens_from_par_test_small = [str(item[0]) for item in test_texts]\n",
    "input_tokens_from_par_train_small = [str(item[1]) for item in train_texts]\n",
    "input_tokens_from_par_val_small = [str(item[1]) for item in val_texts]\n",
    "input_tokens_from_par_test_small = [str(item[1]) for item in test_texts]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
