{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This is the code used to tune the hyperparameters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tune\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhyperopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HyperOptSearch\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschedulers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ASHAScheduler\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ray'"
     ]
    }
   ],
   "source": [
    "#Libaries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pb\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n",
    "from ray import tune\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import math\n",
    "\n",
    "#Modell och tokenizer\n",
    "model_name = \"AI-Sweden-Models/gpt-sw3-126m\"\n",
    "task_name = \"testNamn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,padding_side = 'left')\n",
    "def model_init():\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, return_dict=True,)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_init().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes away all columns that have less than one procent of its boxes filled. \n",
    "def taBortEnProcent(FromFile, PlaceToSave):\n",
    "    data = pb.read_excel(FromFile)\n",
    "    missing_percentage = (data.isnull().sum() / len(data)) * 100\n",
    "    cols_to_drop = missing_percentage[missing_percentage >= 99].index\n",
    "    datasmall_filtered = data.drop(columns=cols_to_drop)\n",
    "\n",
    "    print(\"Shape after filtering:\", datasmall_filtered.shape)\n",
    "\n",
    "    datasmall_filtered.to_excel(PlaceToSave, index=False)\n",
    "\n",
    "    return datasmall_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes away several keywords and it's sentence in the output texts.   \n",
    "\n",
    "def rensaUtdata(FromFile):\n",
    "    data = FromFile\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    input_tokens = data.iloc[:, 1:].values.tolist()  \n",
    "    output_tokens = data.iloc[:, 0].values.tolist()\n",
    "    output_tokens = [str(value) for value in output_tokens]\n",
    "\n",
    "    processed_texts = []\n",
    "\n",
    "    for text in output_tokens:\n",
    "        SokOrd = {'jmf','Jmf','JMF','jämfört','Jämfört','jämförelse','Jämförelse','tidigare','Tidigare','föregående','Föregående','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022','2023','2024'}\n",
    "\n",
    "        for i in range(0, 3):\n",
    "            for s in SokOrd:\n",
    "                head, sep, tail = text.partition(s)\n",
    "                if head == text:\n",
    "                    head = \"\" \n",
    "                txt = head[::-1]\n",
    "\n",
    "                for tecken in txt:\n",
    "                    if tecken == '.':\n",
    "                        head2, sep2, tail2 = tail.partition('.')\n",
    "                        Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                        new_txt = Second_tail[::-1]\n",
    "                        text = new_txt + tail2\n",
    "                        break\n",
    "                    \n",
    "                    if tecken == '(':\n",
    "                        head2, sep2, tail2 = tail.partition(')')\n",
    "                        Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                        Second_tail = Second_tail[1:] #takes away the last whitespace\n",
    "                        new_txt = Second_tail[::-1]\n",
    "                        text = new_txt + tail2\n",
    "                        break\n",
    "\n",
    "                    if tecken == '?':\n",
    "                        head2, sep2, tail2 = tail.partition('.')\n",
    "                        Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                        Second_tail = Second_tail[1:] #takes away the last whitespace\n",
    "                        new_txt = Second_tail[::-1]\n",
    "                        text = new_txt + tail2\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "        processed_texts.append(text.strip()) \n",
    "\n",
    "    output_tokens = processed_texts\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"output\": output_tokens,\n",
    "        \"input\": input_tokens,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Order all input and output text in to pairs. Then formating them in order for the model to understand.\n",
    "def formatering(indata, utdata):\n",
    "    par_tokens = [(utdata[i], indata[i])for i in range(len(indata))]\n",
    "\n",
    "    train_texts, val_and_test_texts = train_test_split(par_tokens, test_size=0.2) \n",
    "    val_texts, test_texts = train_test_split(val_and_test_texts, test_size=0.5)\n",
    "\n",
    "    output_from_par_train_small = [str(item[0]) for item in train_texts]\n",
    "    output_from_par_val_small = [str(item[0]) for item in val_texts]\n",
    "    input_from_par_train_small = [str(item[1]) for item in train_texts]\n",
    "    input_from_par_val_small = [str(item[1]) for item in val_texts]\n",
    "\n",
    "    formatted_data_train = [f\"<|endoftext|><s>User: Skriv en patientjornal efter en ultraljudsundersökning utifrån dessa värden: {input_token}<s>Bot:{output_token}<s>\" \n",
    "                        for input_token, output_token in zip(input_from_par_train_small, output_from_par_train_small)]\n",
    "\n",
    "    formatted_data_val = [f\"<|endoftext|><s>User: Skriv en patientjornal efter en ultraljudsundersökning utifrån dessa värden: {input_token}<s>Bot:{output_token}<s>\" \n",
    "                        for input_token, output_token in zip(input_from_par_val_small, output_from_par_val_small)]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"train\": formatted_data_train,\n",
    "        \"val\": formatted_data_val,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dataset and tokenize the texts.\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, formatted_data, tokenizer):\n",
    "        self.formatted_data = formatted_data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.formatted_data)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        formatted_data = self.formatted_data[idx]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            formatted_data,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,   \n",
    "            max_length = 2048,      \n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.flatten(),\n",
    "            \"labels\": inputs.input_ids.flatten(),\n",
    "            \"attention_mask\": inputs.attention_mask.flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starts hyparparameter tuning with ASHAS\n",
    "def StartHyperParamTuning(train_data, val_data):\n",
    "\n",
    "    for i in range(0, 3):\n",
    "        for j in range(0, 2):\n",
    "            for s in range(0,3):\n",
    "\n",
    "                if j == 0: activationfunction = \"gelu\"\n",
    "                if j == 1: activationfunction = \"swish\"\n",
    "                \n",
    "                def model_init():\n",
    "                    return AutoModelForCausalLM.from_pretrained(\n",
    "                        model_name, \n",
    "                        return_dict=True,\n",
    "                        n_layer = int(6 * math.pow(2,i)),\n",
    "                        n_head = int(6 * math.pow(2,s)),\n",
    "                        activation_function = activationfunction,                       \n",
    "\n",
    "                    )\n",
    "            \n",
    "                device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "                model_init().to(device)\n",
    "\n",
    "\n",
    "                training_args = TrainingArguments(\n",
    "                \"test\", evaluation_strategy=\"steps\",\n",
    "                eval_steps=10,\n",
    "                do_train=True,\n",
    "                do_eval=True,\n",
    "                per_device_eval_batch_size=4,\n",
    "                per_device_train_batch_size=3,\n",
    "                disable_tqdm=True\n",
    "                )\n",
    "                trainer = Trainer(\n",
    "                model = None,\n",
    "                args=training_args,\n",
    "                tokenizer=tokenizer,\n",
    "                train_dataset=train_data,\n",
    "                eval_dataset=val_data,\n",
    "                model_init=model_init,\n",
    "                )\n",
    "\n",
    "                def ray_hp_space(*args, **kwargs):\n",
    "                    return {\n",
    "                        \"per_device_train_batch_size\": tune.choice([1,2]),\n",
    "                        \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n",
    "                        \"gradient_accumulation_steps\": tune.choice([1,2,3,4]),\n",
    "                        \"num_train_epochs\": tune.choice([1,2,3]),\n",
    "                        \"gradient_checkpointing\" : tune.choice([False]),\n",
    "                        \"weight_decay\": tune.uniform(0.0, 0.2),\n",
    "                        \"fp16\": tune.choice([True, False]),\n",
    "                        \"warmup_ratio\" : tune.uniform(0, 1e-6),\n",
    "                        \"adam_beta1\": tune.uniform(0.4, 0.999999999),\n",
    "                        \"adam_beta2\": tune.uniform(0.4, 0.999999999),\n",
    "                        \"max_grad_norm\": tune.uniform(0.2, 1.5),\n",
    "                        \"fp16_opt_level\": tune.choice([\"O1\", \"O2\"]),\n",
    "                        \"adam_epsilon\": tune.loguniform(1e-10, 1e-5),\n",
    "\n",
    "                    }\n",
    "            \n",
    "\n",
    "                bestParamNow =[{\n",
    "                    \"per_device_train_batch_size\": 1,\n",
    "                    \"learning_rate\": 7.65391e-05,\n",
    "                    \"gradient_accumulation_steps\": 1,\n",
    "                    \"num_train_epochs\": 2,\n",
    "                    \"gradient_checkpointing\" : False,\n",
    "                    \"weight_decay\": 0.0704687,\n",
    "                    \"fp16\": True,\n",
    "                    \"warmup_ratio\": 4.54937e-07,\n",
    "                    \"adam_beta1\": 0.9,\n",
    "                    \"adam_beta2\": 0.999,\n",
    "                    \"max_grad_norm\": 1,\n",
    "                    \"fp16_opt_level\": \"O1\",\n",
    "                    \"adam_epsilon\": 1e-8,\n",
    "                }]\n",
    "\n",
    "                best_trial = trainer.hyperparameter_search(\n",
    "                    direction=\"minimize\",\n",
    "                    hp_space = ray_hp_space,\n",
    "                    backend=\"ray\",\n",
    "                    n_trials=64,\n",
    "                    search_alg=HyperOptSearch(metric=\"eval_loss\", mode=\"min\", points_to_evaluate = bestParamNow),\n",
    "                    scheduler=ASHAScheduler(metric=\"eval_loss\", mode=\"min\"))\n",
    "\n",
    "                print(\"Antal lager: \",  6 * math.pow(2,i))\n",
    "                print(\"Antal heads: \",  6 * math.pow(2,s))\n",
    "                print(\"Activationfunction: \", activationfunction)\n",
    "                print(\"Best Validation Loss: \", best_trial[1])\n",
    "                print(\"Corresponding Hyperparameters: \", best_trial[2])\n",
    "\n",
    "                with open(\"HyperOpti.txt\", \"a\") as file:\n",
    "                    file.write((\"Antal lager: \" + str(6 * math.pow(2,i))) +\"\\n\")\n",
    "                    file.write((\"Antal heads: \" + str(6 * math.pow(2,s))) +\"\\n\")\n",
    "                    file.write(\"Activationfunction: \" +str(activationfunction) + \"\\n\")\n",
    "                    file.write(\"Best Validation Loss: \" + str(best_trial[1]) + \"\\n\")\n",
    "                    file.write(\"Corresponding Hyperparameters: \" + str(best_trial[2]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process för att träna en modell\n",
    "'''\n",
    "As you can see in the following function a new file is saved after columns with only 1 % filled have been taken away.\n",
    "Only needs to be ran once, saves new file to later use\n",
    "Use a small dataset when hypertuning, otherwise it takes a very long time\n",
    "'''\n",
    "file = taBortEnProcent(\"place_of_original_file\", \"place_to_save_file\")\n",
    "utdata = rensaUtdata(file)[\"output\"]\n",
    "indata = rensaUtdata(file)[\"input\"]\n",
    "train_texter = formatering(indata, utdata)[\"train\"]\n",
    "val_texter = formatering(indata, utdata)[\"val\"]\n",
    "\n",
    "train_dataset = MyDataset(train_texter, tokenizer)\n",
    "val_dataset = MyDataset(train_texter, tokenizer)\n",
    "\n",
    "StartHyperParamTuning(train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
