{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libaries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pb\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n",
    "\n",
    "#Modell och tokenizer\n",
    "model_name = \"AI-Sweden-Models/gpt-sw3-126m\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes away all columns that have less than one procent of its boxes filled. \n",
    "def taBortEnProcent(FromFile, PlaceToSave):\n",
    "    data = pb.read_excel(FromFile)\n",
    "    missing_percentage = (data.isnull().sum() / len(data)) * 100\n",
    "    cols_to_drop = missing_percentage[missing_percentage >= 99].index\n",
    "    data_filtered = data.drop(columns=cols_to_drop)\n",
    "\n",
    "    print(\"Shape after filtering:\", data_filtered.shape)\n",
    "\n",
    "    data_filtered.to_excel(PlaceToSave, index=False)\n",
    "\n",
    "    return data_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes away several keywords and it's sentence in the output texts.   \n",
    "\n",
    "def rensaUtdata(FromFile):\n",
    "    data = FromFile\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    input_tokens = data.iloc[:, 1:].values.tolist()  \n",
    "    output_tokens = data.iloc[:, 0].values.tolist()\n",
    "    output_tokens = [str(value) for value in output_tokens]\n",
    "\n",
    "    processed_texts = []\n",
    "\n",
    "    for text in output_tokens:\n",
    "        SokOrd = {'jmf','Jmf','JMF','jämfört','Jämfört','jämförelse','Jämförelse','tidigare','Tidigare','föregående','Föregående','2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022','2023','2024'}\n",
    "\n",
    "        for i in range(0, 3):\n",
    "            for s in SokOrd:\n",
    "                head, sep, tail = text.partition(s)\n",
    "                if head == text:\n",
    "                    head = \"\" \n",
    "                txt = head[::-1]\n",
    "\n",
    "                for tecken in txt:\n",
    "                    if tecken == '.':\n",
    "                        head2, sep2, tail2 = tail.partition('.')\n",
    "                        Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                        new_txt = Second_tail[::-1]\n",
    "                        text = new_txt + tail2\n",
    "                        break\n",
    "                    \n",
    "                    if tecken == '(':\n",
    "                        head2, sep2, tail2 = tail.partition(')')\n",
    "                        Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                        Second_tail = Second_tail[1:] #takes away the last whitespace\n",
    "                        new_txt = Second_tail[::-1]\n",
    "                        text = new_txt + tail2\n",
    "                        break\n",
    "\n",
    "                    if tecken == '?':\n",
    "                        head2, sep2, tail2 = tail.partition('.')\n",
    "                        Second_head, Second_sep, Second_tail = txt.partition(tecken)\n",
    "                        Second_tail = Second_tail[1:] #takes away the last whitespace\n",
    "                        new_txt = Second_tail[::-1]\n",
    "                        text = new_txt + tail2\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "        processed_texts.append(text.strip()) \n",
    "\n",
    "    output_tokens = processed_texts\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"output\": output_tokens,\n",
    "        \"input\": input_tokens,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Order all input and output text in to pairs. Then formating them in order for the model to understand.\n",
    "def formatering(indata, utdata):\n",
    "    par_tokens = [(utdata[i], indata[i])for i in range(len(indata))]\n",
    "\n",
    "    train_texts, val_and_test_texts = train_test_split(par_tokens, test_size=0.2) \n",
    "    val_texts, test_texts = train_test_split(val_and_test_texts, test_size=0.5)\n",
    "\n",
    "    output_from_par_train_small = [str(item[0]) for item in train_texts]\n",
    "    output_from_par_val_small = [str(item[0]) for item in val_texts]\n",
    "    output_from_par_test_small = [str(item[0]) for item in test_texts]\n",
    "    input_from_par_train_small = [str(item[1]) for item in train_texts]\n",
    "    input_from_par_val_small = [str(item[1]) for item in val_texts]\n",
    "    input_from_par_test_small = [str(item[1]) for item in test_texts]\n",
    "\n",
    "    formatted_data_train = [f\"<|endoftext|><s>User: Skriv en patientjornal efter en ultraljudsundersökning utifrån dessa värden: {input_token}<s>Bot:{output_token}<s>\" \n",
    "                        for input_token, output_token in zip(input_from_par_train_small, output_from_par_train_small)]\n",
    "\n",
    "    formatted_data_val = [f\"<|endoftext|><s>User: Skriv en patientjornal efter en ultraljudsundersökning utifrån dessa värden: {input_token}<s>Bot:{output_token}<s>\" \n",
    "                        for input_token, output_token in zip(input_from_par_val_small, output_from_par_val_small)]\n",
    "\n",
    "    formatted_data_test = [f\"<|endoftext|><s>User: Skriv en patientjornal efter en ultraljudsundersökning utifrån dessa värden: {input_token}<s>Bot:{output_token}<s>\" \n",
    "                        for input_token, output_token in zip(input_from_par_test_small, output_from_par_test_small)]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"train\": formatted_data_train,\n",
    "        \"val\": formatted_data_val,\n",
    "        \"test\": formatted_data_test\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dataset and tokenize the texts.\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, formatted_data, tokenizer):\n",
    "        self.formatted_data = formatted_data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.formatted_data)\n",
    "\n",
    "    def __getitem__(self, idx):        \n",
    "        formatted_data = self.formatted_data[idx]\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            formatted_data,\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,   \n",
    "            max_length = 2048,      \n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.flatten(),\n",
    "            \"labels\": inputs.input_ids.flatten(),\n",
    "            \"attention_mask\": inputs.attention_mask.flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starts a training-loop and after saves the model and tokenizer to a desierd file.\n",
    "\n",
    "def StartaTrain(model, tokenizer, train_dataset, val_dataset, PlaceToSave):\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"test_trainer\", \n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=31100,\n",
    "        per_device_train_batch_size= 1,\n",
    "        learning_rate= 7.65391e-05,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        num_train_epochs= 3,\n",
    "        gradient_checkpointing= False,\n",
    "        weight_decay= 0.0704687,\n",
    "        fp16= True,\n",
    "        warmup_ratio= 4.54937e-07,\n",
    "        adam_beta1= 0.9,\n",
    "        adam_beta2= 0.999,\n",
    "        max_grad_norm= 1.0,\n",
    "        fp16_opt_level= 'O1',\n",
    "        adam_epsilon= 1e-08,\n",
    "        logging_steps=31100,\n",
    "        save_strategy= \"steps\",\n",
    "        save_steps=31100,\n",
    "        logging_dir=(\"./logs\"),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        tokenizer= tokenizer,\n",
    "        args = training_args,\n",
    "        train_dataset= train_dataset,\n",
    "        eval_dataset= val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(PlaceToSave)\n",
    "    tokenizer.save_pretrained(PlaceToSave)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process för att träna en modell\n",
    "'''\n",
    "As you can see in the below functione a new file is saved after 1 % have been taken away.\n",
    "So if you alredy have doen this function one time, its more time efficent\n",
    "to just loade the new file directly\n",
    "'''\n",
    "file = taBortEnProcent(\"From_this_file\", \"To_this_file\")\n",
    "utdata = rensaUtdata(file)[\"output\"]\n",
    "indata = rensaUtdata(file)[\"input\"]\n",
    "train_texter = formatering(indata, utdata)[\"train\"]\n",
    "val_texter = formatering(indata, utdata)[\"val\"]\n",
    "test_texter = formatering(indata, utdata)[\"test\"]\n",
    "\n",
    "train_dataset = MyDataset(train_texter, tokenizer)\n",
    "val_dataset = MyDataset(train_texter, tokenizer)\n",
    "\n",
    "StartaTrain(model, tokenizer, train_dataset, val_dataset, \"Place_to_save\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The folowing code is to generate an answer from the AI. We have created a seperate file for this but the code\n",
    "is a litle bit diffrent when you use the same dataset as when you train.\n",
    "\n",
    "So this code should be used for investigation purposes when you dont want or have\n",
    "the time to load a new dataset. The other file, \"Kex_AI_Generating\" is designed to be used\n",
    "when you want too create several texts. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating(text, model, tokenizer, device):\n",
    "    index_bot = text.find(\"Bot\")\n",
    "    new_string = text[:index_bot + 4]\n",
    "    prompt = new_string.strip()\n",
    "\n",
    "\n",
    "    token_count = len(tokenizer.encode_plus(prompt)[\"input_ids\"])\n",
    "    max_token_count = 2048 - 140\n",
    "\n",
    "    if token_count > max_token_count:\n",
    "        # Hitta indexet där de tre sista elementen börjar\n",
    "        end_index = len(prompt) - 7\n",
    "        end_seq = prompt[end_index:]\n",
    "        tokens = tokenizer.encode_plus(prompt[:end_index])[\"input_ids\"]\n",
    "        tokens = tokens[:max_token_count]\n",
    "        prompt = (tokenizer.decode(tokens) + (end_seq)).strip()\n",
    "    \n",
    "\n",
    "    generator = pipeline('text-generation', tokenizer=tokenizer, model=model, device=device)\n",
    "    generated = generator(prompt, max_new_tokens=140, do_sample=True, temperature=0.47, top_p=1, top_k = 23, repetition_penalty = 1.05)[0][\"generated_text\"]\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process to generate text\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/d1/KEX/.myenv/GPT-sw3-356m-BaseTest_myown\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/mnt/d1/KEX/.myenv/GPT-sw3-356m-BaseTest_myown\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "file = pb.read_excel(\"LitenData_rensad.xlsx\")\n",
    "in_data = rensaUtdata(file)[\"input\"]\n",
    "out_data = rensaUtdata(file)[\"output\"]\n",
    "test_dataset = formatering(in_data, out_data)[\"test\"]\n",
    "idx = 3\n",
    "generated_text = generating(test_dataset[idx], model, tokenizer, device)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
